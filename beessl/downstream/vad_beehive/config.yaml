# Path: beessl/downstream/vad_beehive/config.yaml
# Data manifest
expdir: !PLACEHOLDER
data_root: !PLACEHOLDER # You can replace this by hand
model_folder: !ref <expdir>/save
log_path: !ref <expdir>/log.txt
win_length: 0.25
sample_rate: 16000

# logs
train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
  save_file: !ref <log_path>

# Training Parameters
number_of_epochs: 100
batch_size: 128
embd_dim: 64

dataloader_options:
  shuffle: True
  batch_size: !ref <batch_size>

augmentation: !new:speechbrain.lobes.augment.TimeDomainSpecAugment
  sample_rate: 16000
  speeds: [98, 102]
  drop_chunk_count_low: 0
  drop_chunk_count_high: 1
  drop_chunk_length_low: 320
  drop_chunk_length_high: 1600

num_hidden_layers: !PLACEHOLDER # Automatically computed! Do not change
in_features: !PLACEHOLDER # Automatically computed! Do not change

featurer_projector: !new:torch.nn.Linear
  in_features: !ref <in_features>
  out_features: !ref <embd_dim>

featurizer: !new:beessl.downstream.featurizer.Featurizer
  num_hidden_layers: !ref <num_hidden_layers>

pooling: !new:speechbrain.lobes.models.ECAPA_TDNN.AttentiveStatisticsPooling
  channels: !ref <embd_dim>
  attention_channels: 16
  global_context: True

in_projector: !applyref:operator.add
  - !ref <embd_dim>
  - !ref <embd_dim>

projector: !new:speechbrain.nnet.containers.Sequential
  input_shape: [null, !ref <in_projector>]
  drop1: !new:torch.nn.Dropout
    p: 0.5
  linear: !new:torch.nn.Linear
    in_features: !ref <in_projector>
    out_features: !ref <embd_dim>
  bn: !new:torch.nn.BatchNorm1d
    num_features: !ref <embd_dim>
  drop2: !new:torch.nn.Dropout
    p: 0.25
  activation: !new:torch.nn.ReLU
  linear2: !new:torch.nn.Linear
    in_features: !ref <embd_dim>
    out_features: 1

loss: !new:torch.nn.BCEWithLogitsLoss
  pos_weight: !new:torch.Tensor
    data: [8.0]

modules:
  featurer_projector: !ref <featurer_projector>
  featurizer: !ref <featurizer>
  pooling: !ref <pooling>
  projector: !ref <projector>

error_stats: !name:speechbrain.utils.metric_stats.BinaryMetricStats

# Optimizer and Scheduler parameters
lr: 0.0001
opt_class: !name:torch.optim.AdamW
  lr: !ref <lr>
  weight_decay: 0.00005

lr_scheduler: !new:speechbrain.nnet.schedulers.NewBobScheduler
  initial_value: !ref <lr>
  annealing_factor: 0.5
  patient: 1

# Save state of the training process
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
  checkpoints_dir: !ref <model_folder>
  recoverables:
    featurer_projector: !ref <featurer_projector>
    featurizer: !ref <featurizer>
    pooling: !ref <pooling>
    projector: !ref <projector>
    epoch_counter: !ref <epoch_counter>

epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter
  limit: !ref <number_of_epochs>
